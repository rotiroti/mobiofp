{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MoBioFP - Fingerphoto Recognition (Fingertip Object Detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "from mobiofp.utils import (\n",
    "    quality_scores,\n",
    "    fingertip_enhancement,\n",
    "    fingertip_thresholding,\n",
    "    fingerprint_mapping,\n",
    "    fingerprint_enhancement,\n",
    "    imkpts,\n",
    "    orb_flann_matcher,\n",
    ")\n",
    "from mobiofp.background import BackgroundRemoval\n",
    "\n",
    "from shared import read_images, save_images, show_images, show_iqa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define global constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_DIR = \"../data/raw/samples\"\n",
    "PROCESSED_DIR = \"../data/processed/samples/detection\"\n",
    "\n",
    "# Assume the model is already downloaded and placed in the models directory.\n",
    "# Use one of the following models based on your system architecture.\n",
    "\n",
    "# MODEL_CHECKPOINT = \"../models/fingertip-obj-amd64.pt\" # For AMD64\n",
    "MODEL_CHECKPOINT = \"../models/fingertip-obj-arm64.pt\"  # For ARM64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, images_titles = read_images(SAMPLE_DIR, rotate=True, rotate_angle=90)\n",
    "show_images(images, images_titles, fig_size=15, sup_title=\"Sample Fingerphoto Images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fingertip detection using YOLOv8n pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(MODEL_CHECKPOINT)\n",
    "model.info()\n",
    "results = model(images, stream=True, max_det=1)\n",
    "\n",
    "predicted_images = []\n",
    "predicted_images_titles = []\n",
    "bbox_coords = []\n",
    "fingertip_images = []\n",
    "fingertip_images_titles = []\n",
    "\n",
    "for result, title in zip(results, images_titles):\n",
    "    boxes = result.boxes.xyxy.tolist()\n",
    "    if not boxes:\n",
    "        continue\n",
    "    boxes = [int(coord) for coord in boxes[0]]\n",
    "    bbox_coords.append(boxes)\n",
    "\n",
    "    original = result.orig_img\n",
    "    x1, y1, x2, y2 = boxes\n",
    "    fingertip = original[y1:y2, x1:x2]\n",
    "    fingertip_images.append(fingertip)\n",
    "    fingertip_images_titles.append(title)\n",
    "\n",
    "    predicted = result.plot()\n",
    "    predicted_images.append(predicted)\n",
    "    predicted_images_titles.append(title)\n",
    "\n",
    "show_images(\n",
    "    predicted_images, predicted_images_titles, fig_size=15, sup_title=\"YOLOv8n Fingertip Detection\"\n",
    ")\n",
    "show_images(fingertip_images, fingertip_images_titles, sup_title=\"Fingertip Images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_images(\n",
    "    [cv2.cvtColor(image, cv2.COLOR_RGB2BGR) for image in fingertip_images],\n",
    "    fingertip_images_titles,\n",
    "    PROCESSED_DIR + \"/fingertips\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remover = BackgroundRemoval()\n",
    "\n",
    "# Remove background from the cropped images\n",
    "fingertip_masks = [remover.apply(fingertip) for fingertip in fingertip_images]\n",
    "\n",
    "show_images(fingertip_masks, fingertip_images_titles, sup_title=\"Fingertip Masks\")\n",
    "save_images(fingertip_masks, fingertip_images_titles, PROCESSED_DIR + \"/masks\", file_extesion=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fingertip Image-Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharpness_scores = []\n",
    "contrast_scores = []\n",
    "mask_coverage_scores = []\n",
    "\n",
    "for image, mask in zip(fingertip_images, fingertip_masks):\n",
    "    sharpness_score, contrast_score, mask_coverage_scorere = quality_scores(image, mask)\n",
    "    sharpness_scores.append(sharpness_score)\n",
    "    contrast_scores.append(contrast_score)\n",
    "    mask_coverage_scores.append(mask_coverage_scorere)\n",
    "\n",
    "show_iqa(sharpness_scores, contrast_scores, mask_coverage_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of images before filtering: {len(fingertip_images)}\")\n",
    "print(f\"Number of masks before filtering: {len(fingertip_masks)}\")\n",
    "\n",
    "# NOTE:\n",
    "#\n",
    "# For the scope of this demonstration, we will consider only the binary mask coverage score\n",
    "# and we will not consider the sharpness and contrast scores. The binary mask coverage threshold\n",
    "# is set to 70% only to include all the images in the dataset since the dataset is already too small\n",
    "# and we want to demonstrate the next steps of the pipeline.\n",
    "BINARY_MASK_COVERAGE_THRESH = 70.0\n",
    "\n",
    "iqa_images = []\n",
    "iqa_masks = []\n",
    "iqa_titles = []\n",
    "\n",
    "for mcs, fingertip, fingertip_mask, fingertip_title in zip(\n",
    "    mask_coverage_scores, fingertip_images, fingertip_masks, fingertip_images_titles\n",
    "):\n",
    "    if mcs >= BINARY_MASK_COVERAGE_THRESH:\n",
    "        iqa_images.append(fingertip)\n",
    "        iqa_masks.append(fingertip_mask)\n",
    "        iqa_titles.append(fingertip_title)\n",
    "\n",
    "assert len(iqa_images) == len(iqa_masks) == len(iqa_titles)\n",
    "\n",
    "print(f\"Number of images after filtering: {len(iqa_images)}\")\n",
    "print(f\"Number of masks after filtering: {len(iqa_masks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fingertip Enhancement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray_images = [cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) for image in iqa_images]\n",
    "gray_images = [\n",
    "    cv2.bitwise_and(image, image, mask=mask) for image, mask in zip(gray_images, iqa_masks)\n",
    "]\n",
    "\n",
    "show_images(gray_images, iqa_titles, cmap=\"gray\", sup_title=\"Grayscale Fingertip Images\")\n",
    "save_images(gray_images, iqa_titles, PROCESSED_DIR + \"/grayscale\", file_extesion=\"jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fingertip_enhanced_images = [fingertip_enhancement(image) for image in gray_images]\n",
    "\n",
    "show_images(fingertip_enhanced_images, iqa_titles, sup_title=\"Fingertip Enhanced Images\")\n",
    "save_images(\n",
    "    fingertip_enhanced_images, iqa_titles, PROCESSED_DIR + \"/enhancement\", file_extesion=\"png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fingertip Binarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fingertip_thresh = [fingertip_thresholding(image) for image in fingertip_enhanced_images]\n",
    "\n",
    "show_images(fingertip_thresh, iqa_titles, cmap=\"gray\", sup_title=\"Fingertip Thresholded Images\")\n",
    "save_images(fingertip_thresh, iqa_titles, PROCESSED_DIR + \"/binarized\", file_extesion=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fingertip to Fingeprint Conversion\n",
    "\n",
    "The function `fingerprint_mapping()` takes an fingertip-enhanced and converts it into a fingerprint image.\n",
    "It does this by:\n",
    "\n",
    "- Resizing the image.\n",
    "- Calculating the local gradient of the image using Sobel filters.\n",
    "- Calculating the orientation of the ridges in the fingerprint.\n",
    "- Extracting a region of the image and smoothing it to reduce noise.\n",
    "- Calculating the x-signature of the region and finding its local maxima to estimate the ridge period.\n",
    "- Creating a bank of Gabor filters with different orientations.\n",
    "- Filtering the image with each filter in the bank.\n",
    "- Assembling the final result by taking the corresponding convolution result for each pixel based on the closest orientation in the Gabor bank.\n",
    "- Converting the result to grayscale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fingerprints, fingerprint_titles = [], []\n",
    "\n",
    "for image, title in zip(fingertip_thresh, iqa_titles):\n",
    "    fingerprint = fingerprint_mapping(image)\n",
    "    if fingerprint is not None:\n",
    "        fingerprints.append(fingerprint)\n",
    "        fingerprint_titles.append(title)\n",
    "\n",
    "show_images(fingerprints, fingerprint_titles, sup_title=\"Fingerprint Images\", fig_size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fingerprint_enhanced_images = [fingerprint_enhancement(fingerprint) for fingerprint in fingerprints]\n",
    "\n",
    "show_images(fingerprint_enhanced_images, iqa_titles, sup_title=\"Fingerprint Enhanced Images\")\n",
    "save_images(\n",
    "    fingerprint_enhanced_images, iqa_titles, PROCESSED_DIR + \"/mapping\", file_extesion=\"png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probe vs Gallery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_images, probe_titles = zip(\n",
    "    *[\n",
    "        (image, title)\n",
    "        for image, title in zip(fingerprint_enhanced_images, fingerprint_titles)\n",
    "        if \"n\" in title\n",
    "    ]\n",
    ")\n",
    "gallery_images, gallery_titles = zip(\n",
    "    *[\n",
    "        (image, title)\n",
    "        for image, title in zip(fingerprint_enhanced_images, fingerprint_titles)\n",
    "        if \"w\" in title\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Skeletonize the fingerprint images\n",
    "probe_images = [cv2.ximgproc.thinning(image) for image in probe_images]\n",
    "gallery_images = [cv2.ximgproc.thinning(image) for image in gallery_images]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction - ORB (Oriented FAST and Rotated BRIEF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ORB detector\n",
    "orb = cv2.ORB_create()\n",
    "\n",
    "# Find the keypoints and descriptors for the probe images\n",
    "probe_orb_keypoints, probe_orb_descriptors = zip(\n",
    "    *[orb.detectAndCompute(image, None) for image in probe_images]\n",
    ")\n",
    "\n",
    "# Show keypoints for each probe image\n",
    "probe_orb_keypoints_images = [\n",
    "    imkpts(image, keypoints, cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)\n",
    "    for image, keypoints in zip(probe_images, probe_orb_keypoints)\n",
    "]\n",
    "\n",
    "# Show keypoints for each probe image\n",
    "show_images(\n",
    "    probe_orb_keypoints_images,\n",
    "    probe_titles,\n",
    "    fig_size=20,\n",
    "    sup_title=\"Probe ORB Keypoints\",\n",
    ")\n",
    "\n",
    "# Find the keypoints and descriptors for the gallery images\n",
    "gallery_orb_keypoints, gallery_orb_descriptors = zip(\n",
    "    *[orb.detectAndCompute(image, None) for image in gallery_images]\n",
    ")\n",
    "\n",
    "# Show keypoints for each gallery image\n",
    "gallery_orb_keypoints_images = [\n",
    "    imkpts(image, keypoints, cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)\n",
    "    for image, keypoints in zip(gallery_images, gallery_orb_keypoints)\n",
    "]\n",
    "\n",
    "# Show keypoints for each probe image\n",
    "show_images(\n",
    "    gallery_orb_keypoints_images,\n",
    "    gallery_titles,\n",
    "    fig_size=20,\n",
    "    sup_title=\"Gallery ORB Keypoints\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Matching - FLANN based matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_matrix = {}\n",
    "\n",
    "for p_img, p_title, p_kpts, p_desc in zip(\n",
    "    probe_images, probe_titles, probe_orb_keypoints, probe_orb_descriptors\n",
    "):\n",
    "    distance_matrix[p_title] = {}\n",
    "\n",
    "    for g_img, g_title, g_kpts, g_desc in zip(\n",
    "        gallery_images, gallery_titles, gallery_orb_keypoints, gallery_orb_descriptors\n",
    "    ):\n",
    "        values = {}\n",
    "        distance, matches_img = orb_flann_matcher(\n",
    "            p_img, p_kpts, p_desc, g_img, g_kpts, g_desc, include_img=True\n",
    "        )\n",
    "        if np.isnan(distance) or np.isinf(distance):\n",
    "            distance = -1\n",
    "        values[\"distance\"] = distance\n",
    "        values[\"matches_img\"] = matches_img\n",
    "        distance_matrix[p_title][g_title] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_images = []\n",
    "matched_images_titles = []\n",
    "\n",
    "for p_title in distance_matrix:\n",
    "    for g_title in distance_matrix[p_title]:\n",
    "        p_subject, p_illumination, p_finger, p_background, _ = p_title.split(\"_\")\n",
    "        g_subject, g_illumination, g_finger, g_background, _ = g_title.split(\"_\")\n",
    "\n",
    "        if p_subject == g_subject and p_finger == g_finger:\n",
    "            matched_images.append(distance_matrix[p_title][g_title][\"matches_img\"])\n",
    "            title = f\"Probe: ({p_subject}, {p_illumination}, {p_finger}, {p_background}) = {distance_matrix[p_title][g_title]['distance']:.2f}\"\n",
    "            matched_images_titles.append(\n",
    "                f\"{p_title} vs {g_title} = {distance_matrix[p_title][g_title]['distance']:.2f}\"\n",
    "            )\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "for i, (image, title) in enumerate(zip(matched_images, matched_images_titles), 1):\n",
    "    plt.suptitle(\"Matched Image (distances)\", fontsize=24)\n",
    "    plt.subplot(2, 2, i)\n",
    "    plt.imshow(image)\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jolene3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
