{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MoBioFP - Fingertip Segmentation using U-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env SM_FRAMEWORK=tf.keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imported libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import albumentations as A\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import platform\n",
    "\n",
    "# TODO: this should be removed as already part of tensorflow.keras\n",
    "import keras\n",
    "\n",
    "# TODO: this should be removed as already part of tensorflow.keras\n",
    "import segmentation_models as sm\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    Conv2D,\n",
    "    MaxPooling2D,\n",
    "    concatenate,\n",
    "    Conv2DTranspose,\n",
    "    BatchNormalization,\n",
    "    Activation,\n",
    "    Dropout,\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import ndimage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(**images):\n",
    "    \"\"\"PLot images in one row.\"\"\"\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(\" \".join(name.split(\"_\")).title())\n",
    "        plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def make_prediction(model, image, shape):\n",
    "    \"\"\"\n",
    "    Makes a prediction using a trained model on an image.\n",
    "\n",
    "    Parameters:\n",
    "    model (keras.Model): The trained model to use for prediction.\n",
    "    image (str): The path to the image file to predict on.\n",
    "    shape (tuple): The target size to resize the image to before prediction.\n",
    "\n",
    "    Returns:\n",
    "    np.array: A 2D numpy array representing the predicted mask, reshaped to (256, 256).\n",
    "    \"\"\"\n",
    "    img = img_to_array(load_img(image, target_size=shape))\n",
    "    img = np.expand_dims(img, axis=0) / 255.0\n",
    "    mask = model.predict(img).round()\n",
    "\n",
    "    mask = (mask[0] > 0.5) * 1\n",
    "    # print(np.unique(mask,return_counts=True))\n",
    "    mask = np.reshape(mask, (256, 256))\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "# Get iamge prediction, merge it with original image and diplay or save it\n",
    "def mask_and_segmented_image(image, save=False, output_path=None):\n",
    "    # Load image\n",
    "    original_img = load_img(image)\n",
    "    img = img_to_array(original_img)\n",
    "\n",
    "    # Do prediction and resize mask\n",
    "    mask = make_prediction(model, image, (256, 256, 3))  # TODO: model not defined!\n",
    "    mask2 = cv2.merge([mask, mask, mask]).astype(\"float32\")\n",
    "    mask2 = cv2.resize(mask2, (img.shape[1], img.shape[0]))\n",
    "\n",
    "    # Get segmented image\n",
    "    h, w = img.shape[:2]\n",
    "    mask_resized = cv2.resize(np.uint8(mask * 1), (w, h))\n",
    "    mask_resized = mask_resized != 0\n",
    "    segment = np.zeros((h, w, 3))\n",
    "    segment[:, :, 0] = img[:, :, 0] * mask_resized\n",
    "    segment[:, :, 1] = img[:, :, 1] * mask_resized\n",
    "    segment[:, :, 2] = img[:, :, 2] * mask_resized\n",
    "    segment[np.where((segment == [0, 0, 0]).all(axis=2))] = [0, 0, 0]\n",
    "    img[np.where((img == [255, 255, 255]).all(axis=2))] = [0, 0, 0]\n",
    "\n",
    "    if save:\n",
    "        image_output_path = output_path + \"/{}\".format(os.path.basename(image)).replace(\n",
    "            \".jpg\", \".png\"\n",
    "        )\n",
    "        cv2.imwrite(\n",
    "            image_output_path,\n",
    "            cv2.cvtColor(segment.astype(\"float32\"), cv2.COLOR_RGB2BGR),\n",
    "        )\n",
    "        # plt.imshow(segment/255.)\n",
    "        # plt.waitforbuttonpress()\n",
    "    else:\n",
    "        plt.imshow(segment / 255.0)\n",
    "        # plt.waitforbuttonpress()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for dataset generation, augmentation and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for generating dataset\n",
    "class DataGenerator(Sequence):\n",
    "    \"Generates data for Keras\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        images,\n",
    "        image_dir,\n",
    "        labels,\n",
    "        label_dir,\n",
    "        augmentation=None,\n",
    "        preprocessing=None,\n",
    "        batch_size=8,\n",
    "        dim=(256, 256, 3),\n",
    "        shuffle=True,\n",
    "    ):\n",
    "        \"Initialization\"\n",
    "        self.dim = dim\n",
    "        self.images = images\n",
    "        self.image_dir = image_dir\n",
    "        self.labels = labels\n",
    "        self.label_dir = label_dir\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"Denotes the number of batches per epoch\"\n",
    "        return int(np.floor(len(self.images) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"Generate one batch of data\"\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index * self.batch_size : (index + 1) * self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [k for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"Updates indexes after each epoch\"\n",
    "        self.indexes = np.arange(len(self.images))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        \"Generates data containing batch_size samples\"  # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        batch_imgs = list()\n",
    "        batch_labels = list()\n",
    "\n",
    "        # Generate data\n",
    "        for i in list_IDs_temp:\n",
    "            # degree = np.random.random() * 360\n",
    "            # Store sample\n",
    "            img = load_img(self.image_dir + \"/\" + self.images[i], target_size=self.dim)\n",
    "            img = img_to_array(img) / 255.0\n",
    "\n",
    "            # Store class\n",
    "            label = load_img(\n",
    "                self.label_dir + \"/\" + self.labels[i], target_size=self.dim\n",
    "            )\n",
    "            label = img_to_array(label)[:, :, 0]\n",
    "            label = label != 0\n",
    "            label = ndimage.binary_erosion(ndimage.binary_erosion(label))\n",
    "            label = ndimage.binary_dilation(ndimage.binary_dilation(label))\n",
    "            label = np.expand_dims((label) * 1, axis=2)\n",
    "\n",
    "            # apply augmentations\n",
    "            if self.augmentation:\n",
    "                # TODO: Rename in case random.sample is used\n",
    "                sample = self.augmentation(image=img, mask=label)\n",
    "                img, label = sample[\"image\"], sample[\"mask\"]\n",
    "\n",
    "            # apply preprocessing\n",
    "            if self.preprocessing:\n",
    "                # TODO: Rename in case random.sample is used\n",
    "                sample = self.preprocessing(image=img, mask=label)\n",
    "                img, label = sample[\"image\"], sample[\"mask\"]\n",
    "\n",
    "            batch_imgs.append(img)  # transformed_img\n",
    "            batch_labels.append(label)  # transformed_label\n",
    "\n",
    "        return np.array(batch_imgs, dtype=np.float32), np.array(\n",
    "            batch_labels, dtype=np.float32\n",
    "        )\n",
    "\n",
    "\n",
    "# Some augmentations\n",
    "def round_clip_0_1(x, **kwargs):\n",
    "    \"\"\"\n",
    "    Rounds the input to the nearest integer and clips it to the range [0, 1].\n",
    "\n",
    "    Parameters:\n",
    "    x (np.array): The input array to round and clip.\n",
    "    **kwargs: Arbitrary keyword arguments. This is included to maintain compatibility with the albumentations library, which may pass additional arguments.\n",
    "\n",
    "    Returns:\n",
    "    np.array: The rounded and clipped input array.\n",
    "    \"\"\"\n",
    "    return x.round().clip(0, 1)\n",
    "\n",
    "\n",
    "def get_training_augmentation():\n",
    "    \"\"\"\n",
    "    Defines the augmentation pipeline for training data.\n",
    "\n",
    "    Returns:\n",
    "    albumentations.Compose: The augmentation pipeline.\n",
    "    \"\"\"\n",
    "    train_transform = [\n",
    "        A.PadIfNeeded(min_height=256, min_width=256, always_apply=True, border_mode=0),\n",
    "        # Flip augmentations\n",
    "        A.OneOf(\n",
    "            [A.HorizontalFlip(p=1), A.VerticalFlip(p=1), A.Transpose(p=1)],\n",
    "            p=0.9,\n",
    "        ),\n",
    "        # Geometric augmentations\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.ShiftScaleRotate(\n",
    "                    scale_limit=0.3,\n",
    "                    rotate_limit=45,\n",
    "                    shift_limit=0.2,\n",
    "                    border_mode=0,\n",
    "                    p=1,\n",
    "                ),\n",
    "                A.Perspective(p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "        # Resolution augmentation\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.Sharpen(p=1),\n",
    "                A.Blur(blur_limit=3, p=1),\n",
    "                A.MotionBlur(blur_limit=3, p=1),\n",
    "            ],\n",
    "            p=0.9,\n",
    "        ),\n",
    "        # Visual alterations\n",
    "        A.OneOf(\n",
    "            [\n",
    "                # A.GaussNoise(var_limit=(0.0, 0.01), p=1),\n",
    "                A.HueSaturationValue(\n",
    "                    hue_shift_limit=1, sat_shift_limit=0.2, val_shift_limit=0.5, p=1\n",
    "                ),\n",
    "                A.RandomBrightnessContrast(p=1),\n",
    "                # A.RandomGamma(p=1),\n",
    "            ],\n",
    "            p=0.5,\n",
    "        ),\n",
    "        A.Lambda(mask=round_clip_0_1),\n",
    "    ]\n",
    "\n",
    "    return A.Compose(train_transform)\n",
    "\n",
    "\n",
    "def get_validation_augmentation():\n",
    "    \"\"\"\n",
    "    Defines the augmentation pipeline for validation data.\n",
    "\n",
    "    Returns:\n",
    "    albumentations.Compose: The augmentation pipeline.\n",
    "    \"\"\"\n",
    "    test_transform = [A.PadIfNeeded(256, 256)]  # make image shape divisible by 32\n",
    "\n",
    "    return A.Compose(test_transform)\n",
    "\n",
    "\n",
    "def jaccard_distance_loss(y_true, y_pred, smooth=100):\n",
    "    \"\"\"\n",
    "    Calculates the Jaccard distance loss between the true and predicted labels.\n",
    "\n",
    "    Parameters:\n",
    "    y_true (tf.Tensor): The true labels.\n",
    "    y_pred (tf.Tensor): The predicted labels.\n",
    "    smooth (int, optional): A smoothing factor to prevent division by zero. Defaults to 100.\n",
    "\n",
    "    Returns:\n",
    "    tf.Tensor: The Jaccard distance loss.\n",
    "    \"\"\"\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "\n",
    "    return (1 - jac) * smooth\n",
    "\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the Dice coefficient between the true and predicted labels.\n",
    "\n",
    "    Parameters:\n",
    "    y_true (tf.Tensor): The true labels.\n",
    "    y_pred (tf.Tensor): The predicted labels.\n",
    "    smooth (int, optional): A smoothing factor to prevent division by zero. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "    tf.Tensor: The Dice coefficient.\n",
    "    \"\"\"\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "\n",
    "    return (2.0 * intersection + K.epsilon()) / (\n",
    "        K.sum(y_true_f) + K.sum(y_pred_f) + K.epsilon()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE:\n",
    "#\n",
    "# Download the dataset and the model checkpoint from Google Drive and place them in the following directories:\n",
    "#\n",
    "# IMAGE_DIR_PATH = \"../data/raw/iiitd-unet/images\"\n",
    "# MASKS_DIR_PATH = \"../data/raw/iiitd-unet/masks\"\n",
    "# MODEL_CHECKPOINT_PATH = \"../models/best-iiitd-unet.h5\"\n",
    "#\n",
    "# This is a temporary solution and will be replaced by a better one in the future.\n",
    "IMAGE_DIR_PATH = \"../data/raw/iiitd-unet/images\"\n",
    "MASKS_DIR_PATH = \"../data/raw/iiitd-unet/masks\"\n",
    "MODEL_CHECKPOINT_PATH = \"../models/best-iiitd-unet.h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading images and create train-validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_paths = os.listdir(IMAGE_DIR_PATH)\n",
    "masks_paths = os.listdir(MASKS_DIR_PATH)\n",
    "\n",
    "imgs_paths.sort()\n",
    "masks_paths.sort()\n",
    "\n",
    "train_imgs, val_imgs, train_masks, val_masks = train_test_split(\n",
    "    imgs_paths, masks_paths, test_size=0.15, random_state=42\n",
    ")\n",
    "\n",
    "# Check if the dataset is loaded correctly\n",
    "assert len(train_imgs) == len(train_masks) and len(val_imgs) == len(val_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of training images: {len(train_imgs)}\")\n",
    "print(f\"Number of validation images: {len(val_imgs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and validation generator\n",
    "train_generator = DataGenerator(\n",
    "    train_imgs,\n",
    "    IMAGE_DIR_PATH,\n",
    "    train_masks,\n",
    "    MASKS_DIR_PATH,\n",
    "    augmentation=get_training_augmentation(),\n",
    "    preprocessing=None,\n",
    "    batch_size=8,\n",
    "    dim=(256, 256, 3),\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "train_steps = train_generator.__len__()\n",
    "\n",
    "val_generator = DataGenerator(\n",
    "    val_imgs,\n",
    "    IMAGE_DIR_PATH,\n",
    "    val_masks,\n",
    "    MASKS_DIR_PATH,\n",
    "    augmentation=get_validation_augmentation(),\n",
    "    preprocessing=None,\n",
    "    batch_size=8,\n",
    "    dim=(256, 256, 3),\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "val_steps = val_generator.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show random samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = train_generator.__getitem__(1)\n",
    "\n",
    "# TODO: Rename in case random.sample is used\n",
    "sample = random.randint(0, 7)\n",
    "\n",
    "visualize(image=X[sample], label=y[sample])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(tensor, nfilters, size=3, padding=\"same\", initializer=\"he_normal\"):\n",
    "    \"\"\"\n",
    "    Defines a convolutional block with two Conv2D layers, each followed by BatchNormalization and ReLU activation.\n",
    "\n",
    "    Parameters:\n",
    "    tensor (tf.Tensor): The input tensor.\n",
    "    nfilters (int): The number of filters for the Conv2D layers.\n",
    "    size (int, optional): The kernel size for the Conv2D layers. Defaults to 3.\n",
    "    padding (str, optional): The padding method for the Conv2D layers. Defaults to \"same\".\n",
    "    initializer (str, optional): The initializer for the Conv2D layers. Defaults to \"he_normal\".\n",
    "\n",
    "    Returns:\n",
    "    tf.Tensor: The output tensor after applying the convolutional block.\n",
    "    \"\"\"\n",
    "    x = Conv2D(\n",
    "        filters=nfilters,\n",
    "        kernel_size=(size, size),\n",
    "        padding=padding,\n",
    "        kernel_initializer=initializer,\n",
    "    )(tensor)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = Conv2D(\n",
    "        filters=nfilters,\n",
    "        kernel_size=(size, size),\n",
    "        padding=padding,\n",
    "        kernel_initializer=initializer,\n",
    "    )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def deconv_block(tensor, residual, nfilters, size=3, padding=\"same\", strides=(2, 2)):\n",
    "    \"\"\"\n",
    "    Defines a deconvolutional block with a Conv2DTranspose layer followed by a concatenation with the residual, and a convolutional block.\n",
    "\n",
    "    Parameters:\n",
    "    tensor (tf.Tensor): The input tensor.\n",
    "    residual (tf.Tensor): The residual tensor to concatenate with the output of the Conv2DTranspose layer.\n",
    "    nfilters (int): The number of filters for the Conv2DTranspose layer.\n",
    "    size (int, optional): The kernel size for the Conv2DTranspose layer. Defaults to 3.\n",
    "    padding (str, optional): The padding method for the Conv2DTranspose layer. Defaults to \"same\".\n",
    "    strides (tuple, optional): The strides for the Conv2DTranspose layer. Defaults to (2, 2).\n",
    "\n",
    "    Returns:\n",
    "    tf.Tensor: The output tensor after applying the deconvolutional block.\n",
    "    \"\"\"\n",
    "    y = Conv2DTranspose(\n",
    "        nfilters, kernel_size=(size, size), strides=strides, padding=padding\n",
    "    )(tensor)\n",
    "    y = concatenate([y, residual], axis=3)\n",
    "    y = conv_block(y, nfilters)\n",
    "    return y\n",
    "\n",
    "\n",
    "def Unet(h, w, filters):\n",
    "    \"\"\"\n",
    "    Defines the architecture of the U-Net model.\n",
    "\n",
    "    Parameters:\n",
    "    h (int): The height of the input images.\n",
    "    w (int): The width of the input images.\n",
    "    filters (int): The number of filters for the first convolutional block. This number is doubled after each max pooling layer in the encoder part and halved after each deconvolutional block in the decoder part.\n",
    "\n",
    "    Returns:\n",
    "    keras.Model: The U-Net model.\n",
    "    \"\"\"\n",
    "\n",
    "    # down\n",
    "    input_layer = Input(shape=(h, w, 3), name=\"image_input\")\n",
    "    conv1 = conv_block(input_layer, nfilters=filters)\n",
    "    conv1_out = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = conv_block(conv1_out, nfilters=filters * 2)\n",
    "    conv2_out = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    conv3 = conv_block(conv2_out, nfilters=filters * 4)\n",
    "    conv3_out = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    conv4 = conv_block(conv3_out, nfilters=filters * 8)\n",
    "    conv4_out = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "    conv4_out = Dropout(0.5)(conv4_out)\n",
    "    conv5 = conv_block(conv4_out, nfilters=filters * 16)\n",
    "    conv5 = Dropout(0.5)(conv5)\n",
    "\n",
    "    # up\n",
    "    deconv6 = deconv_block(conv5, residual=conv4, nfilters=filters * 8)\n",
    "    deconv6 = Dropout(0.5)(deconv6)\n",
    "    deconv7 = deconv_block(deconv6, residual=conv3, nfilters=filters * 4)\n",
    "    deconv7 = Dropout(0.5)(deconv7)\n",
    "    deconv8 = deconv_block(deconv7, residual=conv2, nfilters=filters * 2)\n",
    "    deconv9 = deconv_block(deconv8, residual=conv1, nfilters=filters)\n",
    "    output_layer = Conv2D(filters=1, kernel_size=(1, 1), activation=\"sigmoid\")(deconv9)\n",
    "\n",
    "    # using sigmoid activation for binary classification\n",
    "    model = Model(inputs=input_layer, outputs=output_layer, name=\"Unet\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the model\n",
    "model = Unet(256, 256, 64)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model and create callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the optimizer and its parameters\n",
    "if platform.system() == \"Darwin\":\n",
    "    # Use the legacy Adam optimizer on M1/M2 Macs\n",
    "    optim = keras.optimizers.legacy.Adam(learning_rate=0.0001)\n",
    "else:\n",
    "    # Use the new Adam optimizer on other platforms\n",
    "    optim = Adam(learning_rate=0.0001)\n",
    "\n",
    "# Compile the model, create checkpoints and define an early stopping\n",
    "model.compile(\n",
    "    optimizer=optim,\n",
    "    loss=jaccard_distance_loss,\n",
    "    metrics=[\n",
    "        dice_coef,\n",
    "        sm.metrics.IOUScore(threshold=0.5),\n",
    "        sm.metrics.FScore(threshold=0.5),\n",
    "        \"accuracy\",\n",
    "    ],\n",
    ")\n",
    "mc = ModelCheckpoint(\n",
    "    mode=\"max\",\n",
    "    filepath=\"top-weights.h5\",\n",
    "    monitor=\"val_dice_coef\",\n",
    "    save_best_only=\"True\",\n",
    "    save_weights_only=\"True\",\n",
    "    verbose=1,\n",
    ")\n",
    "es = EarlyStopping(mode=\"max\", monitor=\"val_dice_coef\", patience=3, verbose=1)\n",
    "\n",
    "callbacks = [\n",
    "    # K.callbacks.LearningRateScheduler(scheduler),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        MODEL_CHECKPOINT_PATH,\n",
    "        save_weights_only=True,\n",
    "        save_best_only=True,\n",
    "        mode=\"min\",\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_steps,\n",
    "    epochs=100,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=val_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation Jaccard loss and Dice coeff.\n",
    "plt.figure(figsize=(30, 5))\n",
    "plt.subplot(121)\n",
    "plt.plot(results.history[\"loss\"])\n",
    "plt.plot(results.history[\"val_loss\"])\n",
    "plt.title(\"Model Jaccard loss\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Train\", \"Validation\"], loc=\"upper left\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(results.history[\"dice_coef\"])\n",
    "plt.plot(results.history[\"val_dice_coef\"])\n",
    "plt.title(\"Model dice coef\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Train\", \"Validation\"], loc=\"upper left\")\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation IoU and F1 score\n",
    "plt.figure(figsize=(30, 5))\n",
    "plt.subplot(121)\n",
    "plt.plot(results.history[\"iou_score\"])\n",
    "plt.plot(results.history[\"val_iou_score\"])\n",
    "plt.title(\"Model IoU score\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Train\", \"Validation\"], loc=\"upper left\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(results.history[\"f1-score\"])\n",
    "plt.plot(results.history[\"val_f1-score\"])\n",
    "plt.title(\"Model F1 score\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Train\", \"Validation\"], loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load best model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best weights and evaluate on validation set\n",
    "model.load_weights(MODEL_CHECKPOINT_PATH)\n",
    "scores = model.evaluate(val_generator)\n",
    "\n",
    "print(\"Loss: {:.5}\".format(scores[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test predict on sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTED_MASK_DIR_PATH = \"../data/raw/iiitd-sample/1_i_1_n_1.jpg\"\n",
    "\n",
    "mask_and_segmented_image(PREDICTED_MASK_DIR_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biovisionenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
